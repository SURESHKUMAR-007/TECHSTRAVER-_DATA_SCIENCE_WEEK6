{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "## GRAMMAR CHECKER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Convert to lowercase and join tokens\n",
    "    cleaned_text = ' '.join([token.text.lower() for token in doc if not token.is_punct])\n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Part-of-Speech (POS) Tagging and Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "    \n",
    "    # Dependency Parsing\n",
    "    dependencies = [(token.text, token.dep_, token.head.text) for token in doc]\n",
    "    \n",
    "    return pos_tags, dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Grammar Rule Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grammar(text):\n",
    "    doc = nlp(text)\n",
    "    errors = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Rule: Detect wrong verb forms\n",
    "        if token.pos_ == \"VERB\" and token.dep_ == \"ROOT\":\n",
    "            subject = [w for w in token.head.lefts if w.dep_ == \"nsubj\"]\n",
    "            if subject:\n",
    "                if subject[0].tag_ == \"NN\" and token.tag_ != \"VBZ\":  # Singular subject should have singular verb (VBZ)\n",
    "                    errors.append(f\"Subject-Verb Agreement Error: '{subject[0].text}' should match with verb '{token.text}'\")\n",
    "                elif subject[0].tag_ == \"NNS\" and token.tag_ == \"VBZ\":  # Plural subject should not have singular verb\n",
    "                    errors.append(f\"Subject-Verb Agreement Error: '{subject[0].text}' should not match with singular verb '{token.text}'\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_errors(text):\n",
    "    # Use TextBlob for basic grammar correction\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Correct spelling and grammatical errors\n",
    "    corrected_text = str(blob.correct())\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Full Grammar Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_checker(text):\n",
    "    print(\"Original Text:\", text)\n",
    "    \n",
    "    # Preprocess the text\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    \n",
    "    # Extract POS and Dependency Features\n",
    "    pos_tags, dependencies = extract_features(cleaned_text)\n",
    "    \n",
    "    print(\"\\nPart-of-Speech Tags:\")\n",
    "    for token, tag in pos_tags:\n",
    "        print(f\"{token}: {tag}\")\n",
    "    \n",
    "    print(\"\\nDependency Parsing:\")\n",
    "    for word, dep, head in dependencies:\n",
    "        print(f\"{word} ({dep}) --> {head}\")\n",
    "    \n",
    "    # Check for grammatical errors\n",
    "    errors = check_grammar(cleaned_text)\n",
    "    if errors:\n",
    "        print(\"\\nDetected Errors:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    else:\n",
    "        print(\"\\nNo grammatical errors found.\")\n",
    "    \n",
    "    # Correct the text\n",
    "    corrected_text = correct_errors(text)\n",
    "    print(\"\\nCorrected Text:\", corrected_text)\n",
    "    \n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Testing the Grammar Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: She go to the park every day. The cats runs fast.\n",
      "\n",
      "Part-of-Speech Tags:\n",
      "she: PRON\n",
      "go: VERB\n",
      "to: ADP\n",
      "the: DET\n",
      "park: NOUN\n",
      "every: DET\n",
      "day: NOUN\n",
      "the: DET\n",
      "cats: NOUN\n",
      "runs: VERB\n",
      "fast: ADV\n",
      "\n",
      "Dependency Parsing:\n",
      "she (nsubj) --> go\n",
      "go (ROOT) --> go\n",
      "to (prep) --> go\n",
      "the (det) --> park\n",
      "park (pobj) --> to\n",
      "every (det) --> day\n",
      "day (npadvmod) --> runs\n",
      "the (det) --> cats\n",
      "cats (nsubj) --> runs\n",
      "runs (advcl) --> go\n",
      "fast (advmod) --> runs\n",
      "\n",
      "No grammatical errors found.\n",
      "\n",
      "Corrected Text: The go to the park every day. The cats runs fast.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"She go to the park every day. The cats runs fast.\"\n",
    "\n",
    "corrected_text = grammar_checker(test_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
